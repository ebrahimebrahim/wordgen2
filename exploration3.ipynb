{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: Use data from [phoible](https://phoible.org/) to learn a useful embedding from phonological feature space to a vector space in which distances between allophones tend to be small. Go to the [phoible project github](https://github.com/phoible/dev/tree/master/data) and grab phoible.csv for this exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class EmbedPhones(nn.Module):\n",
    "\n",
    "    def __init__(self, num_features):\n",
    "        super(EmbedPhones, self).__init__()\n",
    "        \n",
    "        self.res = nn.Linear(num_features,num_features,bias=False)\n",
    "#         identity=torch.from_numpy(np.identity(num_features,dtype=np.dtype('float32')))\n",
    "        with torch.no_grad():\n",
    "            self.res.weight.normal_(0,0.02)\n",
    "#             self.embed.weight += identity\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        return x+self.res(x) # res represents the difference between the embedding map and the identity map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "def add_path_to_local_module(module_name):\n",
    "    module_path = os.path.abspath(os.path.join(module_name))\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n",
    "add_path_to_local_module(\"epitran\")\n",
    "add_path_to_local_module(\"panphon\")\n",
    "\n",
    "import panphon.featuretable\n",
    "ft = panphon.featuretable.FeatureTable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22  features for 'ɛ æ':\n",
      " tensor([[ 1.,  1., -1.,  1., -1., -1., -1.,  0.,  1., -1., -1.,  0., -1.,  0.,\n",
      "         -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "        [ 1.,  1., -1.,  1., -1., -1., -1.,  0.,  1., -1., -1.,  0., -1.,  0.,\n",
      "         -1., -1.,  1., -1., -1., -1.,  1., -1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9673,  0.9900, -1.1050,  1.1218, -0.9670, -0.9498, -1.0924,  0.0088,\n",
       "          1.0072, -1.0245, -0.9193, -0.2126, -0.8851, -0.0915, -1.1130, -0.8540,\n",
       "         -0.9901, -1.0156, -1.0737, -1.0103, -1.0275, -0.8322],\n",
       "        [ 0.9726,  0.9893, -1.0308,  1.1549, -0.9203, -0.9097, -1.0870, -0.0045,\n",
       "          0.9938, -1.0599, -0.9625, -0.2255, -0.9510, -0.1066, -1.0706, -0.8278,\n",
       "          1.0649, -0.9972, -1.0063, -0.9814,  0.9401, -0.8264]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PanphonNoLikey(Exception):\n",
    "    pass\n",
    "\n",
    "class NoAvailableData(Exception):\n",
    "    pass\n",
    "\n",
    "def to_panphon_fts(ipa_symbols):\n",
    "    \"\"\" Convert a string of ipa symbols to a numpy matrix whose rows are features.\n",
    "        Do not include duplicates when panphon identifies symbols in terms of features. \"\"\"\n",
    "    if ipa_symbols == \"NA\" :  raise NoAvailableData(\"The string you gave indicates that there's no allophone data\")\n",
    "    fts = ft.word_to_vector_list(ipa_symbols,numeric=True)\n",
    "    if not fts : raise PanphonNoLikey(\"Panphon does not recognize this IPA symbol\")\n",
    "    fts = np.array(fts) \n",
    "    fts = np.unique(fts,axis=0) # panphon will identify some things; this gets rid of duplicates\n",
    "    fts = fts.astype(np.dtype('float32'))\n",
    "    return torch.from_numpy(fts)\n",
    "\n",
    "feats = to_panphon_fts('ɛ æ')\n",
    "num_features = len(feats[0])\n",
    "print(num_features,\" features for 'ɛ æ':\\n\",feats)\n",
    "emb = EmbedPhones(num_features)\n",
    "emb(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def allophone_data_by_language(num_lists=np.inf):\n",
    "    f = open('phoible.csv')\n",
    "    reader = csv.reader(f)\n",
    "    head = next(reader)\n",
    "    allophones_index = head.index('Allophones')\n",
    "    langkey_index = head.index('ISO6393')\n",
    "    last_langkey = \"not a lang key\"\n",
    "    num_yielded = 0\n",
    "    for i,row in enumerate(reader):\n",
    "        if row[langkey_index] != last_langkey:\n",
    "#             print(\"About to yield data for language\",last_langkey)\n",
    "            last_langkey = row[langkey_index]\n",
    "            if i!=0 and allophone_data:\n",
    "                yield allophone_data\n",
    "                num_yielded += 1\n",
    "            if num_yielded >= num_lists: break\n",
    "            allophone_data = []\n",
    "        try:\n",
    "            allophone_list = to_panphon_fts(row[allophones_index])\n",
    "        except PanphonNoLikey: # We will just skip the entire list of allophones in this case\n",
    "            continue\n",
    "        except NoAvailableData: # We will just skip the entire list of allophones in this case\n",
    "            continue\n",
    "        allophone_data.append(allophone_list)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.word_to_vector_list('˧')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "d = allophone_data_by_language(6)\n",
    "allophone_data=next(d)\n",
    "allophone_data=next(d)\n",
    "allophone_data=next(d)\n",
    "allophone_data=next(d)\n",
    "allophone_data=next(d)\n",
    "allophone_data=next(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distinct_pairs(N):\n",
    "    \"\"\"Generate pairs of integers i<j such that 0 <= i < j < N\"\"\"\n",
    "    for i in range(N):\n",
    "        for j in range(i+1,N):\n",
    "            yield i,j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (0, 2), (0, 3), (1, 2), (1, 3), (2, 3)]\n"
     ]
    }
   ],
   "source": [
    "print(list(distinct_pairs(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqdist(x,y):\n",
    "    \"\"\"Return squared distance between two 1D torch tensors\"\"\"\n",
    "    return ((x-y)**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "emb = EmbedPhones(num_features)\n",
    "lr = 0.001\n",
    "optimizer = optim.Adam(emb.parameters(),lr=lr,weight_decay=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 languages processed. Avg cost:  97.15568542480469\n",
      "50 languages processed. Avg cost:  82.66782123565673\n",
      "100 languages processed. Avg cost:  52.23507556915283\n",
      "150 languages processed. Avg cost:  -7.391268157958985\n",
      "200 languages processed. Avg cost:  -156.40296104431152\n",
      "250 languages processed. Avg cost:  -168.00918228149413\n",
      "300 languages processed. Avg cost:  -258.89240234375\n",
      "350 languages processed. Avg cost:  -244.1227998352051\n",
      "400 languages processed. Avg cost:  -492.7983517456055\n",
      "450 languages processed. Avg cost:  -305.5294819641113\n",
      "500 languages processed. Avg cost:  -541.4068019104004\n",
      "550 languages processed. Avg cost:  -405.8478207397461\n",
      "600 languages processed. Avg cost:  -549.908291015625\n",
      "650 languages processed. Avg cost:  -924.232177734375\n",
      "700 languages processed. Avg cost:  -1054.3478887939452\n",
      "750 languages processed. Avg cost:  -944.8215661621093\n",
      "800 languages processed. Avg cost:  -1793.4756622314453\n",
      "850 languages processed. Avg cost:  -1549.2198699951173\n",
      "900 languages processed. Avg cost:  -4623.1039685058595\n",
      "950 languages processed. Avg cost:  -4771.098626708985\n",
      "1000 languages processed. Avg cost:  -5677.395147705078\n",
      "1050 languages processed. Avg cost:  -2635.508303222656\n",
      "1100 languages processed. Avg cost:  -3600.26615234375\n",
      "1150 languages processed. Avg cost:  -9737.582192382812\n",
      "1200 languages processed. Avg cost:  -2273.490549316406\n",
      "1250 languages processed. Avg cost:  -20265.367630615234\n"
     ]
    }
   ],
   "source": [
    "costs = []\n",
    "for step_num,allophone_data in enumerate(allophone_data_by_language()):\n",
    "\n",
    "    allophone_dists   = 0.\n",
    "    nallophone_dists  = 0. \n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for allophone_list in allophone_data:\n",
    "        for i,j in distinct_pairs(len(allophone_list)):\n",
    "            allophone_dists += sqdist(emb(allophone_list[i]),emb(allophone_list[j]))\n",
    "    for i,j in distinct_pairs(len(allophone_data)):\n",
    "        for x in allophone_data[i]:\n",
    "            for y in allophone_data[j]:\n",
    "                nallophone_dists += sqdist(emb(x),emb(y))\n",
    "\n",
    "    cost = allophone_dists - 0.004 * nallophone_dists\n",
    "    cost.backward()\n",
    "    costs.append(cost.item())\n",
    "    if step_num%50==0:\n",
    "        print(step_num,\"languages processed. Avg cost: \",np.mean(costs))\n",
    "        costs = []\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.0182e+00,  9.4166e-01, -1.0968e+00,  8.4549e-01, -4.9847e-01,\n",
       "         -6.5636e-01, -2.3196e-01, -2.9973e-01,  8.5888e-01, -2.3640e-01,\n",
       "         -4.2198e-01,  5.5916e-02, -1.0287e+00,  2.5039e-01,  2.6265e-03,\n",
       "          7.0186e-02,  6.9011e-01,  6.1996e-01,  7.4508e-01,  9.1607e-02,\n",
       "          6.3379e-01,  6.4222e-01],\n",
       "        [ 1.0985e+00,  4.6997e-01, -9.8451e-01,  7.1055e-01, -3.9627e-01,\n",
       "         -1.7702e-01, -2.9615e-01, -2.2340e-01,  7.0490e-01, -2.2850e-01,\n",
       "         -3.6341e-01,  1.9348e-02, -9.3686e-01,  2.3067e-01,  1.7408e-02,\n",
       "          1.0393e-01,  6.0077e-01,  5.2175e-01,  6.8409e-01,  7.9226e-02,\n",
       "          6.3031e-01,  6.2870e-01],\n",
       "        [-1.0943e+00, -8.4286e-01,  9.7359e-01, -7.8524e-01,  4.4673e-01,\n",
       "          5.5415e-01,  2.2207e-01,  2.3174e-01, -8.3662e-01,  2.3422e-01,\n",
       "          3.8584e-01, -2.8522e-02,  9.8579e-01, -2.5758e-01, -3.9224e-02,\n",
       "         -8.2661e-02, -6.9468e-01, -5.9977e-01, -6.9885e-01, -8.9505e-02,\n",
       "         -6.6109e-01, -6.5677e-01],\n",
       "        [ 1.1228e+00,  8.9368e-01, -1.1097e+00,  2.6642e-01, -3.2890e-01,\n",
       "         -1.5446e-01, -2.8184e-01, -1.0848e-01,  7.5484e-01, -2.0636e-01,\n",
       "         -3.6230e-01,  2.4833e-01, -9.7636e-01, -1.0909e-01, -3.3892e-01,\n",
       "         -2.3294e-02,  5.8901e-01,  5.1384e-01,  6.7338e-01,  5.9836e-02,\n",
       "          6.6651e-01,  5.6703e-01],\n",
       "        [ 1.3192e+00,  9.1401e-01, -1.2406e+00,  7.4737e-01, -4.5462e-01,\n",
       "         -2.5520e-01, -2.4262e-01, -2.3662e-01,  8.3690e-01, -2.0493e-01,\n",
       "         -4.1599e-01,  7.1251e-02, -1.0932e+00,  1.9480e-01, -7.8957e-02,\n",
       "          9.6392e-03,  4.5861e-01,  5.3169e-01,  7.2343e-01,  8.2357e-02,\n",
       "          6.2652e-01,  6.7198e-01],\n",
       "        [-1.3142e+00, -9.0932e-01,  1.1905e+00, -6.9624e-01,  3.6753e-01,\n",
       "          2.2241e-01,  1.9785e-01,  1.2153e-01, -8.4016e-01,  1.8129e-01,\n",
       "          3.5548e-01,  1.9837e-02,  1.1081e+00, -2.8401e-01, -4.4017e-01,\n",
       "         -1.8401e-01, -5.5689e-01, -5.8454e-01, -6.6531e-01, -1.0399e-01,\n",
       "         -6.3510e-01, -6.6018e-01],\n",
       "        [ 9.1860e-01,  1.6724e-01, -8.8742e-01,  4.0901e-01, -4.0126e-01,\n",
       "         -3.6167e-01, -2.9242e-01,  4.2097e-02,  4.7557e-01, -1.2006e-01,\n",
       "         -2.0717e-01, -7.3403e-02, -9.3437e-01,  2.7069e-01,  5.3693e-01,\n",
       "          2.4110e-01,  3.8928e-01,  4.3902e-01,  4.3615e-01,  9.6782e-02,\n",
       "          3.7988e-01,  4.2799e-01],\n",
       "        [ 1.2991e+00,  9.4722e-01, -1.1849e+00,  7.9401e-01, -4.9964e-01,\n",
       "         -6.4799e-01, -2.3220e-01, -1.9876e-01,  8.5545e-01, -2.1800e-01,\n",
       "         -4.4557e-01,  7.5289e-02, -1.1549e+00,  2.7282e-01,  3.4049e-01,\n",
       "          1.4932e-01,  6.6515e-01,  6.0578e-01,  6.8014e-01,  9.9154e-02,\n",
       "          6.4568e-01,  6.6538e-01],\n",
       "        [ 1.1375e+00,  7.9383e-01, -1.0835e+00,  6.8982e-01, -3.9794e-01,\n",
       "         -3.3671e-01, -9.0379e-02, -1.9327e-01,  3.0844e-01, -1.4310e-01,\n",
       "         -3.3274e-01,  6.5781e-02, -1.0036e+00,  2.5084e-01,  2.1114e-01,\n",
       "          1.5925e-01,  6.1711e-01,  5.4249e-01,  6.7178e-01,  8.8647e-02,\n",
       "          5.8344e-01,  6.2923e-01],\n",
       "        [-1.2290e+00, -8.8029e-01,  1.1894e+00, -6.9756e-01,  4.7856e-01,\n",
       "          5.7141e-01,  1.8468e-01,  1.3459e-01, -7.2539e-01, -1.4306e-01,\n",
       "          3.6404e-01,  5.1894e-04,  1.0618e+00, -2.6873e-01, -3.0318e-01,\n",
       "         -1.3514e-01, -6.3558e-01, -5.8526e-01, -6.5871e-01, -1.0591e-01,\n",
       "         -6.2196e-01, -6.3511e-01],\n",
       "        [-1.2309e+00, -9.2639e-01,  1.2119e+00, -7.3625e-01,  4.8871e-01,\n",
       "          5.3561e-01,  2.0702e-01,  1.4260e-01, -8.0658e-01,  1.8097e-01,\n",
       "          9.1690e-02, -1.6391e-02,  1.1158e+00, -2.7315e-01, -2.6946e-01,\n",
       "         -1.3435e-01, -6.5026e-01, -5.8269e-01, -6.7414e-01, -9.8915e-02,\n",
       "         -6.2758e-01, -6.6730e-01],\n",
       "        [ 1.1280e+00,  7.5882e-01, -1.0054e+00,  7.2528e-01, -2.8313e-01,\n",
       "         -1.8622e-01, -2.0693e-01, -7.7241e-02,  7.2354e-01, -1.2177e-01,\n",
       "         -2.8557e-01, -4.0937e-01, -7.6818e-01,  4.8610e-01, -6.5229e-02,\n",
       "          2.0469e-01,  4.8914e-01,  5.0144e-01,  5.3668e-01,  2.7925e-02,\n",
       "          5.4863e-01,  5.5350e-01],\n",
       "        [-1.1696e+00, -8.4653e-01,  1.0423e+00, -6.6860e-01,  5.9335e-01,\n",
       "          6.5917e-01,  1.1911e-01, -4.9745e-02, -8.0603e-01,  1.6594e-01,\n",
       "          3.3292e-01,  4.0734e-02,  8.1793e-01, -3.1435e-01, -5.5543e-01,\n",
       "         -2.0509e-01, -6.2384e-01, -6.1021e-01, -6.8355e-01, -1.1588e-01,\n",
       "         -6.2009e-01, -6.3596e-01],\n",
       "        [ 1.2238e+00,  8.9370e-01, -1.2311e+00,  4.0078e-01, -5.5033e-01,\n",
       "         -6.0392e-01, -1.7074e-01, -1.2876e-01,  8.3514e-01, -1.7025e-01,\n",
       "         -3.4357e-01,  1.8350e-01, -1.0193e+00, -1.5935e-01,  3.9608e-01,\n",
       "          2.0906e-01,  5.9603e-01,  6.0362e-01,  6.3944e-01,  9.2090e-02,\n",
       "          6.1515e-01,  6.0220e-01],\n",
       "        [-7.6741e-01, -5.4420e-01,  6.8409e-01, -6.4201e-01, -1.4603e-01,\n",
       "         -4.2794e-01,  3.4465e-01,  4.0943e-01, -3.7945e-01,  1.4721e-01,\n",
       "          3.3797e-01, -1.1794e-01, -5.6689e-01,  3.5477e-01,  3.1496e-01,\n",
       "          2.8799e-01, -3.7395e-01,  2.4484e-01, -2.0194e-01,  1.0834e-01,\n",
       "         -3.4024e-01, -3.6763e-01],\n",
       "        [-5.2744e-01, -4.1162e-01,  5.8952e-01, -4.1357e-01, -1.5109e-01,\n",
       "         -4.7338e-02,  2.4752e-01,  1.7257e-01, -3.1166e-01,  8.6780e-02,\n",
       "          1.6299e-01,  1.2920e-01,  1.3918e-01,  1.5967e-01,  3.8693e-01,\n",
       "         -2.5291e-01, -2.8298e-01, -3.5187e-01, -4.1025e-01, -1.0103e-03,\n",
       "         -2.2105e-01, -3.0375e-01],\n",
       "        [ 1.2467e+00,  9.4285e-01, -1.1931e+00,  7.8712e-01, -4.4915e-01,\n",
       "         -3.7368e-01, -2.5547e-01, -2.3025e-01,  8.3908e-01, -2.1295e-01,\n",
       "         -4.2746e-01,  7.2432e-02, -1.1208e+00,  2.3202e-01, -4.8731e-02,\n",
       "          2.8615e-02,  1.8011e-01,  5.3940e-01,  7.2296e-01,  9.0955e-02,\n",
       "          5.2407e-01,  6.4367e-01],\n",
       "        [ 1.1443e+00,  8.4513e-01, -1.0659e+00,  6.6605e-01, -4.2709e-01,\n",
       "         -4.3199e-01, -2.0376e-01, -9.6182e-02,  7.7640e-01, -1.6558e-01,\n",
       "         -3.3188e-01,  1.1386e-01, -9.6240e-01,  3.1687e-01,  4.6355e-01,\n",
       "         -9.7124e-03,  4.2299e-01,  1.1247e-01,  3.6990e-01,  1.0868e-01,\n",
       "          5.6978e-01,  6.1732e-01],\n",
       "        [ 1.1732e+00,  9.0459e-01, -1.1000e+00,  7.3270e-01, -3.6872e-01,\n",
       "         -6.2341e-01, -1.9941e-01, -1.7550e-01,  8.2910e-01, -1.8945e-01,\n",
       "         -4.0067e-01, -1.1781e-02, -1.0819e+00,  2.0819e-01,  1.0392e-01,\n",
       "          5.7541e-02,  5.6292e-01,  3.4157e-01,  2.6585e-01,  8.0409e-02,\n",
       "          5.4659e-01,  6.4418e-01],\n",
       "        [-1.2714e+00, -9.6032e-01,  1.2286e+00, -7.9818e-01,  4.9988e-01,\n",
       "          5.3730e-01,  2.3977e-01,  1.9804e-01, -8.4862e-01,  2.0565e-01,\n",
       "          4.3304e-01, -8.6384e-02,  1.0898e+00, -2.4326e-01, -1.8150e-01,\n",
       "         -1.0971e-01, -6.6510e-01, -5.8298e-01, -7.0445e-01, -1.1374e-01,\n",
       "         -6.5828e-01, -6.6709e-01],\n",
       "        [ 1.1557e+00,  9.3592e-01, -1.1574e+00,  8.0010e-01, -4.8321e-01,\n",
       "         -5.7533e-01, -2.4814e-01, -2.4323e-01,  7.8896e-01, -2.0702e-01,\n",
       "         -4.2930e-01,  7.7209e-02, -1.0909e+00,  2.2281e-01,  2.4175e-02,\n",
       "          1.1391e-01,  4.1798e-01,  5.3838e-01,  6.5473e-01,  9.0542e-02,\n",
       "          9.8992e-02,  5.8637e-01],\n",
       "        [ 1.1668e+00,  9.2698e-01, -1.1423e+00,  7.3619e-01, -4.8134e-01,\n",
       "         -5.9012e-01, -2.0989e-01, -1.9921e-01,  8.2939e-01, -1.9211e-01,\n",
       "         -4.0827e-01,  4.3151e-02, -1.0844e+00,  2.3842e-01,  2.0066e-01,\n",
       "          1.1481e-01,  6.0718e-01,  5.8668e-01,  6.8512e-01,  9.6456e-02,\n",
       "          5.6538e-01,  2.1673e-01]], requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.res.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(x,y):\n",
    "    \"\"\"distance between two feature vectors (1d torch tensors) after embedding\"\"\"\n",
    "    return torch.sqrt(sqdist(emb(x),emb(y))).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Epitran with language code ind-Latn... success!\n",
      "72.43220520019531 1.4635822772979736\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from wordgen import *\n",
    "wg = WordgenLearned(3,\"ind-Latn\")\n",
    "\n",
    "ipa_chars = wg.load_ipa_chars('eng-Latn')\n",
    "\n",
    "dists = []\n",
    "for c1 in ipa_chars:\n",
    "    for c2 in ipa_chars:\n",
    "        s1,s2 = to_panphon_fts(c1),to_panphon_fts(c2)\n",
    "        assert(len(s1)==1)\n",
    "        assert(len(s2)==1)\n",
    "        s1,s2 = s1[0],s2[0]\n",
    "        if c1 != c2 : dists.append(dist(s1,s2))\n",
    "max_dist,min_dist = max(dists),min(dists)\n",
    "print(max_dist,min_dist)\n",
    "plt.hist(dists)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipa_chars = list(wg.load_ipa_chars('eng-Latn')) # now this is an ordered list, kind of serving as an enumeration\n",
    "num_chars = len(ipa_chars)\n",
    "projection = [n for n in range(num_chars)] # we start with identity mapping and will gradually identify things\n",
    "# think of projection as mapping from indices representing ipa_chars to equivalence classes\n",
    "# the number of equivalence classes is len(set(projection))\n",
    "M = 20\n",
    "step_size = (max_dist-min_dist)/float((len(ipa_chars)-M)*500)\n",
    "spread = step_size/2.\n",
    "for r0 in np.arange(min_dist,max_dist,step_size):\n",
    "    r = np.random.normal(r0,spread)\n",
    "    ipa_char_index = np.random.randint(num_chars)\n",
    "    s0 = to_panphon_fts(ipa_chars[ipa_char_index])\n",
    "    assert(len(s0)==1)\n",
    "    s0 = s0[0]\n",
    "    for n in range(num_chars):\n",
    "        s = to_panphon_fts(ipa_chars[n])\n",
    "        assert(len(s)==1)\n",
    "        s = s[0]\n",
    "        if dist(s,s0)<r:\n",
    "            projection[n]=projection[ipa_char_index]\n",
    "    if len(set(projection))<=M: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['j']\n",
      "['ɑ', 'o', 'a']\n",
      "['l']\n",
      "['ɾ']\n",
      "['n', 'ʒ', 'z', 'ð']\n",
      "['v', 'ɡ', 'm']\n",
      "['t͡ʃ', 't']\n",
      "['ɛ', 'ɪ']\n",
      "['p']\n",
      "['h', 'ʔ']\n",
      "['n̩']\n",
      "['e', 'i', 'ɹ̩', 'ə']\n",
      "['ŋ']\n",
      "['w']\n",
      "['ʊ', 'æ', 'ɔ', 'ʌ']\n",
      "['ɹ']\n",
      "['m̩']\n",
      "['u']\n",
      "['d͡ʒ', 'd', 's', 'ʃ', 'θ']\n",
      "['f', 'k', 'b']\n"
     ]
    }
   ],
   "source": [
    "for p in set(projection):\n",
    "    print([ipa_chars[n] for n in range(num_chars) if projection[n]==p])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next: Make data loading stochastic, train for multiple epochs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
