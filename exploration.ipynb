{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do this to get wikipedia's latest dump of english language data:\n",
    "\n",
    "``` wget http://download.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2 ```\n",
    "\n",
    "It's going to be a very large file, several gigabytes.\n",
    "\n",
    "Replace \"en\" by the appropriate [language code](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) to grab it for a different language.\n",
    "\n",
    "Run \n",
    "\n",
    "```WikiExtractor.py -cb 250K -o extracted itwiki-latest-pages-articles.xml.bz2```\n",
    "\n",
    "to get a cleaned up version.\n",
    "\n",
    "Alternatively, there is some pre-cleaned text available for download at the [polyglot project](https://sites.google.com/site/rmyeid/projects/polyglot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_words(line):\n",
    "    line = line.lower()\n",
    "    words = line.split(' ')\n",
    "    words = [word.strip(' .()!;\\n') for word in words]\n",
    "    words = list(filter(lambda w:w.isalpha(),words))\n",
    "    return words\n",
    "\n",
    "words=[]\n",
    "with open(\"peuptext.txt\") as f:\n",
    "    for line in f.readlines():\n",
    "        words = words + extract_words(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "glyphs = set(c for word in words for c in word)\n",
    "glyphs.add('WORD_START')\n",
    "glyphs.add('WORD_END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_glyphs = len(glyphs)\n",
    "int_to_glyph = dict(enumerate(glyphs))\n",
    "glyph_to_int = {v:k for k,v in int_to_glyph.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.zeros((num_glyphs,num_glyphs,num_glyphs,num_glyphs),dtype=np.dtype('u8'))\n",
    "for word in words:\n",
    "    for i in range(len(word)+1):\n",
    "        c1 = glyph_to_int['WORD_START'] if i-3<0 else glyph_to_int[word[i-3]]\n",
    "        c2 = glyph_to_int['WORD_START'] if i-2<0 else glyph_to_int[word[i-2]]\n",
    "        c3 = glyph_to_int['WORD_START'] if i-1<0 else glyph_to_int[word[i-1]]\n",
    "        c4 = glyph_to_int['WORD_END'] if i>=len(word) else glyph_to_int[word[i]]\n",
    "        counts[c1,c2,c3,c4] += 1\n",
    "totals = counts.sum(axis=3)\n",
    "distribution = counts / (np.vectorize(lambda x : x if x!=0 else 1)(totals[:,:,:,np.newaxis]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word(dist):\n",
    "    c1 = c2 = c3 = glyph_to_int['WORD_START']\n",
    "    word = []\n",
    "    while c3!=glyph_to_int['WORD_END']:\n",
    "        if distribution[c1,c2,c3].sum()==0:\n",
    "            next_char = np.random.choice(range(num_glyphs))\n",
    "        else:\n",
    "            next_char = np.random.choice(range(num_glyphs),p=distribution[c1,c2,c3])\n",
    "        c1=c2\n",
    "        c2=c3\n",
    "        c3=next_char\n",
    "        word.append(next_char)\n",
    "    return ''.join(int_to_glyph[c] for c in word[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'not'"
      ]
     },
     "execution_count": 582,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_word(distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "filename = 'english' #filename without txt extension\n",
    "\n",
    "# First build alphabet. Right now this is done here by hand to suit english; e.g. no unicode.\n",
    "glyphs = set(map(chr,range(ord('a'),ord('z')+1)))\n",
    "glyphs.add('WORD_START')\n",
    "glyphs.add('WORD_END')\n",
    "num_glyphs = len(glyphs)\n",
    "int_to_glyph = dict(enumerate(glyphs))\n",
    "glyph_to_int = {v:k for k,v in int_to_glyph.items()}\n",
    "\n",
    "\n",
    "def extract_words(line):\n",
    "    line = line.lower()\n",
    "    words = line.split(' ')\n",
    "    words = [word.strip(' .()!;\\n') for word in words]\n",
    "    words = list(filter(lambda w:w and all(c in glyphs for c in w),words))\n",
    "    # TODO: make that filter smart enough to handle unicode\n",
    "    return words\n",
    "\n",
    "\n",
    "# Intitalize counts\n",
    "counts = np.zeros((num_glyphs,num_glyphs,num_glyphs,num_glyphs),dtype=np.dtype('u8'))\n",
    "\n",
    "# Now go through file and build up distributon\n",
    "with open(filename+\".txt\") as f:\n",
    "    for line in f.readlines():\n",
    "        for word in extract_words(line):\n",
    "            for i in range(len(word)+1):\n",
    "                c1 = glyph_to_int['WORD_START'] if i-3<0 else glyph_to_int[word[i-3]]\n",
    "                c2 = glyph_to_int['WORD_START'] if i-2<0 else glyph_to_int[word[i-2]]\n",
    "                c3 = glyph_to_int['WORD_START'] if i-1<0 else glyph_to_int[word[i-1]]\n",
    "                c4 = glyph_to_int['WORD_END'] if i>=len(word) else glyph_to_int[word[i]]\n",
    "                counts[c1,c2,c3,c4] += 1\n",
    "                \n",
    "totals = counts.sum(axis=3)\n",
    "distribution = counts / (np.vectorize(lambda x : x if x!=0 else 1)(totals[:,:,:,np.newaxis]))\n",
    "\n",
    "with open(filename+\".pkl\",'wb') as pickle_file:\n",
    "    pickle.dump(distribution,pickle_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threeditincree\n",
      "mexile\n",
      "raphich\n",
      "technoloursion\n",
      "histicler\n",
      "chameried\n",
      "soutcomple\n",
      "capabicy\n",
      "expanic\n",
      "sections\n",
      "againto\n",
      "expres\n",
      "listrian\n",
      "spitates\n",
      "dryderent\n",
      "himselm\n",
      "profess\n",
      "ordater\n",
      "jourisked\n",
      "execulturked\n",
      "braises\n",
      "perferen\n",
      "intracts\n",
      "providual\n",
      "comple\n",
      "thring\n",
      "acquadrogenerallecreately\n",
      "roducessue\n",
      "showevelsh\n",
      "comporay\n",
      "decifictined\n",
      "heavincelleged\n",
      "aboverst\n",
      "confinable\n",
      "lawyers\n",
      "brituded\n",
      "zanonment\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    word = generate_word(distribution)\n",
    "    if len(word)>5:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
