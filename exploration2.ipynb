{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Segment [+syl, +son, -cons, +cont, -delrel, -lat, -nas, 0strid, +voi, -sg, -cg, 0ant, -cor, 0distr, -lab, -hi, +lo, +back, -round, -velaric, +tense, +long]>\n",
      "<Segment [+syl, +son, -cons, +cont, -delrel, -lat, -nas, 0strid, +voi, -sg, -cg, 0ant, -cor, 0distr, -lab, -hi, +lo, +back, -round, -velaric, +tense, -long]>\n",
      "0.25\n"
     ]
    }
   ],
   "source": [
    "import os,sys\n",
    "def add_path_to_local_module(module_name):\n",
    "    module_path = os.path.abspath(os.path.join(module_name))\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n",
    "add_path_to_local_module(\"epitran\")\n",
    "add_path_to_local_module(\"panphon\")\n",
    "\n",
    "import panphon.featuretable\n",
    "ft = panphon.featuretable.FeatureTable()\n",
    "a_long,a_short = ft.word_fts('aː ă')\n",
    "print(a_long)\n",
    "print(a_short)\n",
    "print(a_long.weighted_distance(a_short))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os,sys\n",
    "\n",
    "def add_path_to_local_module(module_name):\n",
    "    module_path = os.path.abspath(os.path.join(module_name))\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n",
    "add_path_to_local_module(\"epitran\")\n",
    "add_path_to_local_module(\"panphon\")\n",
    "\n",
    "\n",
    "import panphon\n",
    "import panphon.distance\n",
    "import epitran\n",
    "import pickle\n",
    "\n",
    "from wordgen import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Epitran with language code ind-Latn... success!\n"
     ]
    }
   ],
   "source": [
    "wg = WordgenLearned(3,\"ind-Latn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wg.learn_distribution(\"slice.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wg.generate_word()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are interested in generating a \"phonology\" for a fictional language. This constitutes an equivalence relation on the set of IPA symbols. Then generate an orthography to display things. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebrahim/wordgen2/panphon/panphon/distance.py:73: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  dogol_prime = yaml.load(f.read())\n"
     ]
    }
   ],
   "source": [
    "ft=panphon.FeatureTable()\n",
    "dst=panphon.distance.Distance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.125, 1.1875)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dst.weighted_feature_edit_distance('k','ɡ'),dst.weighted_feature_edit_distance('k','d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epi = epitran.Epitran(\"eng-Latn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pɑɹti'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epi.transliterate(\"party\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = panphon.featuretable.FeatureTable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1],\n",
       "       [ 1],\n",
       "       [ 1],\n",
       "       [-1],\n",
       "       [ 1]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.word_array(['voi'],\"pɑɹti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 1, -1, 0, 1, -1, -1, -1, -1, -1, 0, -1]\n",
      "[1, 1, -1, 1, 0, -1, -1, 0, 1, -1, -1, 0, -1, 0, -1, -1, 1, 1, -1, -1, 1, -1]\n",
      "[-1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 1, 1, -1, -1, 1, -1, 1, 1, -1, 0, -1]\n",
      "[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 1, 1, -1, -1, -1, -1, -1, -1, -1, 0, -1]\n",
      "[1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1, 0, -1, 1, -1, -1, -1, -1, 1, -1]\n"
     ]
    }
   ],
   "source": [
    "for v in ft.word_to_vector_list(\"pɑɹti\",numeric=True): print(list(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('syl', 1.0), ('son', 1.0), ('cons', 1.0), ('cont', 0.5), ('delrel', 0.25), ('lat', 0.25), ('nas', 0.25), ('strid', 0.125), ('voi', 0.125), ('sg', 0.125), ('cg', 0.125), ('ant', 0.25), ('cor', 0.25), ('distr', 0.125), ('lab', 0.25), ('hi', 0.25), ('lo', 0.25), ('back', 0.25), ('round', 0.25), ('velaric', 0.125), ('tense', 0.25), ('long', 0.125)]\n"
     ]
    }
   ],
   "source": [
    "# I think these weights are chosen by panphon authors based on their sense for what matters when it comes\n",
    "# to phonological differences. (Probably based on some empirical knowledge since they are linguists...)\n",
    "print(list(zip(dst.fm.names,dst.fm.weights)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate a phonology, here's my idea:\n",
    "- Pick a random batch of weights like the one shown above. Center your choice on the one above, and draw from a gaussian distribution... somehow pick variance intelligently. The chosen weights will be fixed for the phonology generation.\n",
    "- Pick (randomly? or hard coded from start?) a maximum number of phonemes $M$, like 40 or 50.\n",
    "- Gather all IPA symbols somehow. There's the csv files in panphon/data like ipa_all.csv... but idk if that will get everything the epitrans.transliterate can produce. Instead, you could  go through your directory of saved WordgenLearned objects and union together all the sets of ipa symbols found in there (excluding the start and end tokens). Let's call the resulting set of IPA symbols $S$.\n",
    "- The phonology will be a mapping $S\\rightarrow\\mathbb{N}$. Think of at as an enumeration of equivalence classes. The weights chosen above define a particular distance function $d:S\\times S\\rightarrow \\mathbb{R}$. To generate a phonology:\n",
    "  - Start with an injective mapping $f:S\\rightarrow\\mathbb{N}$.\n",
    "  - Randomly pick an element $x$ of $S$ and randomly choose a radius $r$. The radius will be pulled from a distribution that is fixed beforehand (uniform? gaussian? this will require experimentation to pick).\n",
    "  - For each $y\\in S$ with $d(y,x)<r$, redefine $f(y)$ to be $f(x)$ (i.e. \"identify\" those two sounds).\n",
    "  - Now check how many phonemes there are (i.e. size of range of $f$); if it's $\\leq$ the maximum $M$, then stop. Otherwise pick another $x\\in S$ and repeat the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'k', 'ɪ', 'ʊ', 'w', 'd', 'v', 't͡ʃ', 'e', 'f', 'ŋ', 'l', 'u', 'ʔ', 'd͡ʒ', 'p', 'ɡ', 'b', 'ɹ', 'z', 'h', 'n̩', 'ʒ', 'a', 'i', 's', 'm̩', 'ɔ', 'j', 'ʌ', 'θ', 'n', 'ɹ̩', 'm', 'ð', 'æ', 'ʃ', 'ɛ', 'ɾ', 't', 'ɑ', 'o', 'ə'}\n"
     ]
    }
   ],
   "source": [
    "print(wg.load_ipa_chars('eng-Latn'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'syl'), (1, 'son'), (-1, 'cons'), (1, 'cont'), (-1, 'delrel'), (-1, 'lat'), (-1, 'nas'), (0, 'strid'), (1, 'voi'), (-1, 'sg'), (-1, 'cg'), (0, 'ant'), (-1, 'cor'), (0, 'distr'), (-1, 'lab'), (-1, 'hi'), (1, 'lo'), (1, 'back'), (-1, 'round'), (-1, 'velaric'), (1, 'tense'), (-1, 'long')]\n"
     ]
    }
   ],
   "source": [
    "print(list(\n",
    "    zip(\n",
    "        ft.word_to_vector_list('a',numeric=True)[0],\n",
    "        dst.fm.names,\n",
    ")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Segment [+syl, +son, -cons, +cont, -delrel, -lat, -nas, 0strid, +voi, -sg, -cg, 0ant, -cor, 0distr, -lab, -hi, +lo, +back, -round, -velaric, +tense, -long]>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.word_fts('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a_long,a_short = ft.word_fts('aː ă')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_long.weighted_distance(a_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.75, 0.625)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dst.weighted_feature_edit_distance('a','ɔ'),0.25+0.25+0.125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Segment [+syl, +son, -cons, +cont, -delrel, -lat, -nas, 0strid, +voi, -sg, -cg, 0ant, -cor, 0distr, -lab, -hi, +lo, +back, -round, -velaric, +tense, -long]>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.125,\n",
       " 0.125,\n",
       " 0.125,\n",
       " 0.125,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.125,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.125,\n",
       " 0.25,\n",
       " 0.125]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft = panphon.featuretable.FeatureTable()\n",
    "ft.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ft.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.93480061 1.0743817  0.54472314 0.37755713 0.24315529 0.28724185\n",
      " 0.2771729  0.07627738 0.13799381 0.09026015 0.10449353 0.45035036\n",
      " 0.25430334 0.12561608 0.37187476 0.27879092 0.34488675 0.33441817\n",
      " 0.2644237  0.14337851 0.27465681 0.13424311]\n"
     ]
    }
   ],
   "source": [
    "# Choose random weights based around the ones that were chosen for panphon. \n",
    "# This amounts to a choice of metric on the space of phones\n",
    "weights = np.array([np.exp(np.random.normal(np.log(w),0.25)) for w in ft.weights])\n",
    "weights *= sum(ft.weights)/weights.sum()\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_short.weighted_distance(a_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26848621936619627"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_short.weighted_distance(a_long,weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10.875, 0.25)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipa_chars = wg.load_ipa_chars('eng-Latn')\n",
    "\n",
    "dists = []\n",
    "for c1 in ipa_chars:\n",
    "    for c2 in ipa_chars:\n",
    "        s1,s2 = ft.word_fts(c1),ft.word_fts(c2)\n",
    "        assert(len(s1)==1)\n",
    "        assert(len(s2)==1)\n",
    "        s1,s2 = s1[0],s2[0]\n",
    "        if c1 != c2 : dists.append(s1.weighted_distance(s2))\n",
    "max_dist,min_dist = max(dists),min(dists)\n",
    "max_dist,min_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(dists)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['k', 'ɡ']"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def within_r_of(ipa_char,r,lang_code,weights):\n",
    "    ipa_chars = wg.load_ipa_chars(lang_code)\n",
    "    s1 = ft.word_fts(ipa_char)\n",
    "    assert(len(s1)==1)\n",
    "    s1 = s1[0]\n",
    "    ball = []\n",
    "    for c2 in ipa_chars:\n",
    "        s2 = ft.word_fts(c2)\n",
    "        assert(len(s2)==1)\n",
    "        s2=s2[0]\n",
    "        if s2.weighted_distance(s1,weights) < r : ball.append(c2)\n",
    "    return ball\n",
    "\n",
    "weights = np.array([np.exp(np.random.normal(np.log(w),0.25)) for w in ft.weights])\n",
    "weights *= sum(ft.weights)/weights.sum()\n",
    "within_r_of('ɡ',1.5,'eng-Latn',weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ipa_chars = wg.load_ipa_chars('eng-Latn')\n",
    "M = 20\n",
    "step_size = (max_dist-min_dist)/float((len(ipa_chars)-M)*50)\n",
    "spread = step_size/2.\n",
    "for r0 in np.arange(min_dist,max_dist,step_size):\n",
    "    r = np.random.normal(r0,spread)\n",
    "    print(within_r_of('ɡ',r,'eng-Latn',weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.array([np.exp(np.random.normal(np.log(w),0.25)) for w in ft.weights])\n",
    "weights *= sum(ft.weights)/weights.sum()\n",
    "ipa_chars = list(wg.load_ipa_chars('eng-Latn')) # now this is an ordered list, kind of serving as an enumeration\n",
    "num_chars = len(ipa_chars)\n",
    "projection = [n for n in range(num_chars)] # we start with identity mapping and will gradually identify things\n",
    "# think of projection as mapping from indices representing ipa_chars to equivalence classes\n",
    "# the number of equivalence classes is len(set(projection))\n",
    "M = 20\n",
    "step_size = (max_dist-min_dist)/float((len(ipa_chars)-M)*500)\n",
    "spread = step_size/2.\n",
    "for r0 in np.arange(min_dist,max_dist,step_size):\n",
    "    r = np.random.normal(r0,spread)\n",
    "    ipa_char_index = np.random.randint(num_chars)\n",
    "    s0 = ft.word_fts(ipa_chars[ipa_char_index])\n",
    "    assert(len(s0)==1)\n",
    "    s0 = s0[0]\n",
    "    for n in range(num_chars):\n",
    "        s = ft.word_fts(ipa_chars[n])\n",
    "        assert(len(s)==1)\n",
    "        s = s[0]\n",
    "        if s0.weighted_distance(s,weights)<r:\n",
    "            projection[n]=projection[ipa_char_index]\n",
    "    if len(set(projection))<=M: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['k', 'ɡ']\n",
      "['d', 'z', 'ʒ', 's', 'θ', 'ð', 'ʃ', 't']\n",
      "['m']\n",
      "['w']\n",
      "['ɾ']\n",
      "['t͡ʃ', 'd͡ʒ']\n",
      "['v', 'f']\n",
      "['ɪ', 'ʊ', 'e', 'a', 'i', 'ɔ', 'ʌ', 'æ', 'ɛ', 'ɑ', 'o', 'ə']\n",
      "['ŋ']\n",
      "['l']\n",
      "['u']\n",
      "['ʔ']\n",
      "['p', 'b']\n",
      "['ɹ']\n",
      "['h']\n",
      "['n̩']\n",
      "['m̩']\n",
      "['j']\n",
      "['n']\n",
      "['ɹ̩']\n"
     ]
    }
   ],
   "source": [
    "for p in set(projection):\n",
    "    print([ipa_chars[n] for n in range(num_chars) if projection[n]==p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
