{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os,sys\n",
    "\n",
    "def add_path_to_local_module(module_name):\n",
    "    module_path = os.path.abspath(os.path.join(module_name))\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n",
    "add_path_to_local_module(\"epitran\")\n",
    "add_path_to_local_module(\"panphon\")\n",
    "\n",
    "\n",
    "import panphon\n",
    "import panphon.distance\n",
    "import epitran\n",
    "import pickle\n",
    "\n",
    "from wordgen import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Epitran with language code ind-Latn... success!\n"
     ]
    }
   ],
   "source": [
    "wg = WordgenLearned(3,\"ind-Latn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to learn from slice.txt\n",
      "For each word, each chunk of 3 sounds will be considered.\n",
      "Some of the words that could not be processed will be printed below; just check that nothing too bad is happening.\n",
      "\"[[1]]\" was not processed.\n",
      "\"|\" was not processed.                \n",
      "\".\" was not processed.\n",
      "\",\" was not processed.                \n",
      "\",\" was not processed.\n",
      "\",\" was not processed.\n",
      "\",\" was not processed.\n",
      "\".\" was not processed.\n",
      "\",\" was not processed.                \n",
      "\"(\" was not processed.\n",
      "\":\" was not processed.\n",
      "\"),\" was not processed.\n",
      "\".\" was not processed.\n",
      "\",\" was not processed.                \n",
      "\".\" was not processed.\n",
      "\",\" was not processed.                \n",
      "\";\" was not processed.\n",
      "\",\" was not processed.\n",
      "\".\" was not processed.\n",
      "\".\" was not processed.                \n",
      "\"(\" was not processed.                \n",
      "\")\" was not processed.\n",
      "\"(\" was not processed.\n",
      "\").\" was not processed.\n",
      "\"|\" was not processed.                \n",
      "\"400px\" was not processed.\n",
      "\"|\" was not processed.\n",
      "\"(\" was not processed.\n",
      "\")\" was not processed.\n",
      "\".\" was not processed.\n",
      "[Further output regarding unprocessed words will be suppressed]\n",
      "> 1000 lines processed                   \n"
     ]
    }
   ],
   "source": [
    "wg.learn_distribution(\"slice.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'an'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wg.generate_word()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are interested in generating a \"phonology\" for a fictional language. This constitutes an equivalence relation on the set of IPA symbols. Then generate an orthography to display things. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebrahim/wordgen2/panphon/panphon/distance.py:73: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  dogol_prime = yaml.load(f.read())\n"
     ]
    }
   ],
   "source": [
    "ft=panphon.FeatureTable()\n",
    "dst=panphon.distance.Distance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.125, 1.1875)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dst.weighted_feature_edit_distance('k','ɡ'),dst.weighted_feature_edit_distance('k','d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epi = epitran.Epitran(\"eng-Latn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pɑɹti'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epi.transliterate(\"party\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = panphon.featuretable.FeatureTable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1],\n",
       "       [ 1],\n",
       "       [ 1],\n",
       "       [-1],\n",
       "       [ 1]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.word_array(['voi'],\"pɑɹti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 1, -1, 0, 1, -1, -1, -1, -1, -1, 0, -1]\n",
      "[1, 1, -1, 1, 0, -1, -1, 0, 1, -1, -1, 0, -1, 0, -1, -1, 1, 1, -1, -1, 1, -1]\n",
      "[-1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 1, 1, -1, -1, 1, -1, 1, 1, -1, 0, -1]\n",
      "[-1, -1, 1, -1, -1, -1, -1, 0, -1, -1, -1, 1, 1, -1, -1, -1, -1, -1, -1, -1, 0, -1]\n",
      "[1, 1, -1, 1, -1, -1, -1, 0, 1, -1, -1, 0, -1, 0, -1, 1, -1, -1, -1, -1, 1, -1]\n"
     ]
    }
   ],
   "source": [
    "for v in ft.word_to_vector_list(\"pɑɹti\",numeric=True): print(list(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('syl', 1.0), ('son', 1.0), ('cons', 1.0), ('cont', 0.5), ('delrel', 0.25), ('lat', 0.25), ('nas', 0.25), ('strid', 0.125), ('voi', 0.125), ('sg', 0.125), ('cg', 0.125), ('ant', 0.25), ('cor', 0.25), ('distr', 0.125), ('lab', 0.25), ('hi', 0.25), ('lo', 0.25), ('back', 0.25), ('round', 0.25), ('velaric', 0.25), ('tense', 0.125)]\n"
     ]
    }
   ],
   "source": [
    "# I think these weights are chosen by panphon authors based on their sense for what matters when it comes\n",
    "# to phonological differences. (Probably based on some empirical knowledge since they are linguists...)\n",
    "print(list(zip(dst.fm.names,dst.fm.weights)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate a phonology, here's my idea:\n",
    "- Pick a random batch of weights like the one shown above. Center your choice on the one above, and draw from a gaussian distribution... somehow pick variance intelligently. The chosen weights will be fixed for the phonology generation.\n",
    "- Pick (randomly? or hard coded from start?) a maximum number of phonemes $M$, like 40 or 50.\n",
    "- Gather all IPA symbols somehow. There's the csv files in panphon/data like ipa_all.csv... but idk if that will get everything the epitrans.transliterate can produce. Instead, you could  go through your directory of saved WordgenLearned objects and union together all the sets of ipa symbols found in there (excluding the start and end tokens). Let's call the resulting set of IPA symbols $S$.\n",
    "- The phonology will be a mapping $S\\rightarrow\\mathbb{N}$. Think of at as an enumeration of equivalence classes. The weights chosen above define a particular distance function $d:S\\times S\\rightarrow \\mathbb{R}$. To generate a phonology:\n",
    "  - Start with an injective mapping $f:S\\rightarrow\\mathbb{N}$.\n",
    "  - Randomly pick an element $x$ of $S$ and randomly choose a radius $r$. The radius will be pulled from a distribution that is fixed beforehand (uniform? gaussian? this will require experimentation to pick).\n",
    "  - For each $y\\in S$ with $d(y,x)<r$, redefine $f(y)$ to be $f(x)$ (i.e. \"identify\" those two sounds).\n",
    "  - Now check how many phonemes there are (i.e. size of range of $f$); if it's $\\leq$ the maximum $M$, then stop. Otherwise pick another $x\\in S$ and repeat the process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
