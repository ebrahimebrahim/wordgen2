convert wikipedia language dump for language X into IPA
then learn distributions for IPA
this way we can use arabic, etc.
use https://github.com/dmort27/epitran to do this!

Then you can generate different "projections" of IPA back down into some more normal letters.
A projection is a generated mapping from ipa glyphs to roman letters plus some decorations sometimes (like accented letters, etc.)
There can also be a generated postprocessing which creates weird "spelling rules" 
e.g. i sometimes becomes ee and sometimes beomes ie and sometimes ei.


if you really use IPA, maybe a "glyph memory" of 2 is just as good as a memory of 3 was for english orthography.


so:
start with raw text, like from wikipedia, for the language.
start also with the alphabet, the "glyphs" set for the language.
go through words in file, where a word consists of glyphs of the language only.
convert each word to IPA, and create distribution (memory 2? memory 3?) based on that.
pickle the distribution; this will be saved for the ultimate generator.
do this for a bunch of languages, pickling lots of distributions.
now, to generate a language what we do is
- take a random convex linear combination (maybe coeffs weighted towards bdry of simplex?) of distributions.
  - FIX: A convex linear combination is not a good way to generate something that looks like a language.
    Instead, do the following:
  - Pick which elements of IPA will be in the language. Do this intelligently by picking vowels and consonants separately,
    or by sampling from actual languages and merging results. IDK how to do this part exactly...
  - Create a new distribution D (I will assume here that D is a rank 3 tensor, so we've got a "memory" of 2)
  - For each glyph i,
    - populate D[i,:,:] by stealing the results from a random language.
    - for each glyph j,
      - There's a chance (20%?) to populate D[i,j,:] from another random language
      - for each glyph k,
        - There's a chance (20%?) to populate D[i,j,k] from another random language
      - There's a chance (10%?) to now shuffle the nonzero entries of D[i,j,:]
      - Normalize D[i,j,:] since the k thingy above can screw that up (or do a full renormalization outside all loops).
- look at frequency of phonemes and kill off the ones that are too infrequent, so that there's a reasonable amount of phonemes left
- pickle the resulting distribution, which should be also equipped with a glyphs set and glyphs--int dict.
- generate an IPA projection:
  - each IPA phoneme has a list of roman letters (and roman letters with diacritics) (and strings like 'th') that could represent it,
    in order of "preference" for how strong of a representation it is to the western reader.
  - for each IPA phoneme that appears in the glyphs set for this particular distribution, choose what roman thingy it will project to;
    in your choice make a preference for things that are stronger representations (e.g. avoid diacritics if possible),
    but with a bit of noise so that sometimes you do get a weird character showing up here and there.
  - generate some number (0 or more) of post-processing orthography rules of various types:
    - different representations of the same thing (e.g. i sometimes becomes ee and sometime ie)
    - the same thing, but conditional on being followed or preceded by certain things.
      (you might want to focus on vowels specifically for this sort of thing)
    - silent letters?
  - Now the IPA phoneme to roman mapping and the post processing rules together make up the IPA projection.
- The generated distribution and the generated IPA projection together make up the generated language


Issue with merging system described above:
If there exist i,j s.t. there exists an l s.t. D[l,i,j] is nonzero, but D[i,j,k] is zero for all k, then we have a problem.
We need this not to happen.
So we need that for all i,j,
 either D[l,i,j] vanishes for all l, or
 D[i,j,k] is nonzero for some k. 
So we need that for all i,j,
 If D[l,i,j] is nonzero for some l, then
 D[i,j,k] is nonzero for some k. 

Actually.. you can get super weird feedback loops by merging language distributions this way...



but still need to generate a spelling system somehow...
in fiction spelling conventions do a lot of the work of giving a feel for the "culture"
perhaps have a database of existsing spelling systems and do combinations to generate a new one
maybe some minimal/rare innovation can be allowed




hmm:
is there a difference between generating words forwards vs in reverse vs outwards?



generate a distribution for english, one for french, etc.
then take convex combinations of the distributions to generate a language


figure out nouns, verbs, adj, proper names, etc to generate a distribution for each. then you get a generator for each



once you've generated a language, generate words to go with all english stems
then generate morphology (study morphology to learn how to do this?)
and generate some syntax rules (e.g. SVO vs VSO etc, and all kinds of parameters derived from xbar theory)
then for any given english sentence, you could write its translation in the generated language

hmm, you could also use word embeddings somehow? generate a word embedding? use word embedding to generate morphology? idk...



