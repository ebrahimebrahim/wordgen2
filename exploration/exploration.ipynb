{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do this to get wikipedia's latest dump of english language data:\n",
    "\n",
    "``` wget http://download.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2 ```\n",
    "\n",
    "It's going to be a very large file, several gigabytes.\n",
    "\n",
    "Replace \"en\" by the appropriate [language code](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) to grab it for a different language.\n",
    "\n",
    "Run \n",
    "\n",
    "```WikiExtractor.py -cb 250K -o extracted itwiki-latest-pages-articles.xml.bz2```\n",
    "\n",
    "to get a cleaned up version.\n",
    "\n",
    "Alternatively, there is some pre-cleaned text available for download at the [polyglot project](https://sites.google.com/site/rmyeid/projects/polyglot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_words(line):\n",
    "    line = line.lower()\n",
    "    words = line.split(' ')\n",
    "    words = [word.strip(' .()!;\\n') for word in words]\n",
    "    words = list(filter(lambda w:w.isalpha(),words))\n",
    "    return words\n",
    "\n",
    "words=[]\n",
    "with open(\"peuptext.txt\") as f:\n",
    "    for line in f.readlines():\n",
    "        words = words + extract_words(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "glyphs = set(c for word in words for c in word)\n",
    "glyphs.add('WORD_START')\n",
    "glyphs.add('WORD_END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_glyphs = len(glyphs)\n",
    "int_to_glyph = dict(enumerate(glyphs))\n",
    "glyph_to_int = {v:k for k,v in int_to_glyph.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.zeros((num_glyphs,num_glyphs,num_glyphs,num_glyphs),dtype=np.dtype('u8'))\n",
    "for word in words:\n",
    "    for i in range(len(word)+1):\n",
    "        c1 = glyph_to_int['WORD_START'] if i-3<0 else glyph_to_int[word[i-3]]\n",
    "        c2 = glyph_to_int['WORD_START'] if i-2<0 else glyph_to_int[word[i-2]]\n",
    "        c3 = glyph_to_int['WORD_START'] if i-1<0 else glyph_to_int[word[i-1]]\n",
    "        c4 = glyph_to_int['WORD_END'] if i>=len(word) else glyph_to_int[word[i]]\n",
    "        counts[c1,c2,c3,c4] += 1\n",
    "totals = counts.sum(axis=3)\n",
    "distribution = counts / (np.vectorize(lambda x : x if x!=0 else 1)(totals[:,:,:,np.newaxis]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word(dist):\n",
    "    c1 = c2 = c3 = glyph_to_int['WORD_START']\n",
    "    word = []\n",
    "    while c3!=glyph_to_int['WORD_END']:\n",
    "        if distribution[c1,c2,c3].sum()==0:\n",
    "            next_char = np.random.choice(range(num_glyphs))\n",
    "        else:\n",
    "            next_char = np.random.choice(range(num_glyphs),p=distribution[c1,c2,c3])\n",
    "        c1=c2\n",
    "        c2=c3\n",
    "        c3=next_char\n",
    "        word.append(next_char)\n",
    "    return ''.join(int_to_glyph[c] for c in word[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'not'"
      ]
     },
     "execution_count": 582,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_word(distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "filename = 'indonesian' #filename without txt extension\n",
    "\n",
    "# First build alphabet. Right now this is done here by hand to suit english; e.g. no unicode.\n",
    "glyphs = set(map(chr,range(ord('a'),ord('z')+1)))\n",
    "glyphs.add('WORD_START')\n",
    "glyphs.add('WORD_END')\n",
    "num_glyphs = len(glyphs)\n",
    "int_to_glyph = dict(enumerate(glyphs))\n",
    "glyph_to_int = {v:k for k,v in int_to_glyph.items()}\n",
    "\n",
    "\n",
    "def extract_words(line):\n",
    "    line = line.lower()\n",
    "    words = line.split(' ')\n",
    "    words = [word.strip(' .()!;\\n') for word in words]\n",
    "    words = list(filter(lambda w:w and all(c in glyphs for c in w),words))\n",
    "    # TODO: make that filter smart enough to handle unicode\n",
    "    return words\n",
    "\n",
    "\n",
    "# Intitalize counts\n",
    "counts = np.zeros((num_glyphs,num_glyphs,num_glyphs,num_glyphs),dtype=np.dtype('u8'))\n",
    "\n",
    "# Now go through file and build up distributon\n",
    "with open(filename+\".txt\") as f:\n",
    "    for line in f.readlines():\n",
    "        for word in extract_words(line):\n",
    "            for i in range(len(word)+1):\n",
    "                c1 = glyph_to_int['WORD_START'] if i-3<0 else glyph_to_int[word[i-3]]\n",
    "                c2 = glyph_to_int['WORD_START'] if i-2<0 else glyph_to_int[word[i-2]]\n",
    "                c3 = glyph_to_int['WORD_START'] if i-1<0 else glyph_to_int[word[i-1]]\n",
    "                c4 = glyph_to_int['WORD_END'] if i>=len(word) else glyph_to_int[word[i]]\n",
    "                counts[c1,c2,c3,c4] += 1\n",
    "                \n",
    "totals = counts.sum(axis=3)\n",
    "distribution = counts / (np.vectorize(lambda x : x if x!=0 else 1)(totals[:,:,:,np.newaxis]))\n",
    "\n",
    "with open(filename+\".pkl\",'wb') as pickle_file:\n",
    "    pickle.dump(distribution,pickle_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pasampat\n",
      "ristola\n",
      "emakil\n",
      "yaitu\n",
      "perkeda\n",
      "akhir\n",
      "kesus\n",
      "menemudernuargabupat\n",
      "gerinti\n",
      "pemakan\n",
      "jawahan\n",
      "pangkan\n",
      "sebagai\n",
      "harusatu\n",
      "neksesaika\n",
      "tahuddha\n",
      "karactbolt\n",
      "timan\n",
      "menunjunggu\n",
      "perta\n",
      "terjadi\n",
      "residengang\n",
      "novasihat\n",
      "kolog\n",
      "distusial\n",
      "nilah\n",
      "konortuk\n",
      "untuk\n",
      "menjadirebut\n",
      "masukarena\n",
      "otonia\n",
      "hubunya\n",
      "yangania\n",
      "hammerdiratur\n",
      "kistrikan\n",
      "dangkan\n",
      "berhuburkan\n",
      "singgal\n",
      "tokompat\n",
      "pergannya\n",
      "putinggunadalam\n",
      "katanti\n",
      "provinsteriodeponesia\n",
      "komengur\n",
      "melengijaya\n",
      "mahan\n",
      "padan\n",
      "berang\n",
      "isragai\n",
      "adaudian\n",
      "falatak\n",
      "pemeriorsitakan\n",
      "serinya\n",
      "penyebut\n",
      "seperdayah\n",
      "sandidiri\n",
      "disnia\n",
      "masumbuh\n",
      "telar\n",
      "korenalty\n",
      "memberkemberang\n",
      "sedirang\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    word = generate_word(distribution)\n",
    "    if len(word)>4:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('epitran'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import epitran\n",
    "\n",
    "epi = epitran.Epitran(\"eng-Latn\",ligatures=False)\n",
    "\n",
    "epi.trans_list(\"The cutest thing is really cute!! Potassium. George.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'b',\n",
       " 'd',\n",
       " 'd͡ʑ',\n",
       " 'f',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'ks',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'nd͡ʑ',\n",
       " 'nʥ',\n",
       " 'o',\n",
       " 'p',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 't͡ɕ',\n",
       " 'u',\n",
       " 'w',\n",
       " 'x',\n",
       " 'z',\n",
       " 'ŋ',\n",
       " 'ɕ',\n",
       " 'ə',\n",
       " 'ɡ',\n",
       " 'ɲ',\n",
       " 'ɲt͡ɕ',\n",
       " 'ɲʨ',\n",
       " 'ʑ',\n",
       " 'ʔ',\n",
       " 'ʥ',\n",
       " 'ʨ',\n",
       " '͡'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple = epitran.simple.SimpleEpitran(\"ind-Latn\")\n",
    "ipa_chars=set(p for l in simple._load_g2p_map(\"ind-Latn\",False).values() for p in l)\n",
    "ipa_chars=ipa_chars.union(map(epitran.ligaturize.ligaturize,ipa_chars))\n",
    "for p in list(ipa_chars): ipa_chars.update(set(p))\n",
    "ipa_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'eng-Latn' in epitran.Epitran.special"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import panphon\n",
    "ft = panphon.featuretable.FeatureTable()\n",
    "\n",
    "def load_ipa_chars(lang_code):\n",
    "    \"\"\"Return set of characters that epitran will use for phonemes for the given language code\"\"\"\n",
    "    if lang_code in epitran.Epitran.special:\n",
    "        if lang_code == \"eng-Latn\":\n",
    "            flite  = epitran.flite.Flite()\n",
    "            ipa_chars = set(flite._read_arpabet(\"epitran/epitran/data/arpabet.csv\").values())\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "    else:\n",
    "        simple = epitran.simple.SimpleEpitran(lang_code)\n",
    "        ipa_chars=set(p for l in simple._load_g2p_map(lang_code,False).values() for p in l)\n",
    "#     ipa_chars=set(map(epitran.ligaturize.ligaturize,ipa_chars))\n",
    "#     ipa_chars_single = set()\n",
    "#     for p in ipa_chars: ipa_chars_single.update(set(p))\n",
    "    if '' in ipa_chars : ipa_chars.remove('')\n",
    "    ipa_chars_segmented = set()\n",
    "    for p in ipa_chars: ipa_chars_segmented.update(ft.segs_safe(p))\n",
    "    return ipa_chars_segmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'e', 'ʊ', 's', 'a', 'ə', 'd', 'v', 'z', 'h', 'ɔ', 'ɑ', 't͡ʃ', 'k', 'ʒ', 'j', 'ɡ', 'n', 't', 'm̩', 'f', 'm', 'ŋ', 'n̩', 'ɹ̩', 'i', 'w', 'ð', 'ʔ', 'd͡ʒ', 'ɾ', 'b', 'o', 'θ', 'ɪ', 'ʃ', 'ʌ', 'ɛ', 'p', 'u', 'æ', 'ɹ', 'l'}\n",
      "['a', 'l', 'sˤ', 'b', 'aː', 'ħ']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'uːkiːl'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Hmmm now I am seeing that setting up IPA as glyphs will lose some things.\n",
    "# And not doing so is not so bad for any language that isn't english.\n",
    "# Maybe I'll do IPA for english only?\n",
    "# Then for other languages they just have their own graphemes,\n",
    "# however when we do merging we identify them via a blurry version of IPA\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# idea: generate a phonology at the start.\n",
    "# this contains the \"blur\" i.e. the projection from ipa to a smaller set of phonemes\n",
    "# to help generate this projection, use the phonetic features like place of articulation, etc.\n",
    "\n",
    "import panphon\n",
    "\n",
    "ft = panphon.featuretable.FeatureTable()\n",
    "\n",
    "print(load_ipa_chars('eng-Latn'))\n",
    "\n",
    "print(ft.segs_safe(\"alsˤbaːħ\"))\n",
    "epi = epitran.Epitran(\"ara-Arab\")\n",
    "epi.transliterate(\"وكيل\") # The README for epitran warns against using arabic and some other languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('epitran'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import epitran\n",
    "\n",
    "\n",
    "filename = 'peuptext'  #filename without txt extension\n",
    "lang_code = \"eng-Latn\" #language code for epitran\n",
    "\n",
    "epi = epitran.Epitran(lang_code)\n",
    "\n",
    "\n",
    "\n",
    "# TODO: Now I want a more automated way to build the alphabet of IPA glyphs...\n",
    "# Hmm you should be able to check if something is an IPA character by collecting all IPA characters\n",
    "# (including ligatures? if you use this)\n",
    "# Some IPA characters can be gathered from epitran's maps that are used for simple phonetic things\n",
    "# English is exceptional. To get those, use Flite._read_arpabet\n",
    "\n",
    "glyphs = load_ipa_chars(lang_code)\n",
    "glyphs.add('WORD_START')\n",
    "glyphs.add('WORD_END')\n",
    "num_glyphs = len(glyphs)\n",
    "int_to_glyph = dict(enumerate(glyphs))\n",
    "glyph_to_int = {v:k for k,v in int_to_glyph.items()}\n",
    "\n",
    "\n",
    "def extract_words(line):\n",
    "#     line = line.lower()\n",
    "    words = []\n",
    "    for word in line.split(' '):\n",
    "        orig_word = word # testing line\n",
    "        word = word.strip(' .()!:;,\\n')\n",
    "        word = epi.trans_list(word)\n",
    "        if word and all(c in glyphs for c in word): words.append(word)\n",
    "        else : print(orig_word) # testing line. do report *some* of these in full thing.\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOM;ekindof\n",
      "EXA33mple.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['ð', 'ɪ', 's'],\n",
       " ['ɪ', 'z'],\n",
       " ['ə'],\n",
       " ['l', 'a', 'j', 'n'],\n",
       " ['æ', 'n', 'd'],\n",
       " ['ɪ', 't'],\n",
       " ['s', 'ɹ̩', 'v', 'z'],\n",
       " ['æ', 'z'],\n",
       " ['ɪ', 'ɡ', 'z', 'æ', 'm', 'p', 'ə', 'l'],\n",
       " ['f', 'ɑ', 'ð', 'ɹ̩']]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_words(\"this is a line And IT SERVES, AS SOM;ekindof EXA33mple. example. father.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "9th.\n",
      "Redick--deputed\n",
      "2d.\n",
      "wit--Washington\n",
      "&\n",
      "\n",
      "\n",
      "10\n",
      "&\n",
      "\n",
      "\n",
      "&\n",
      "men[tioned];\n",
      "&\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "follows--viz.--That\n",
      "re-establishment\n",
      "&\n",
      "Stills--intimating\n",
      "&\n",
      "protection--or\n",
      "&\n",
      "&\n",
      "&\n",
      "&\n",
      "\n",
      "\n",
      "recitals--on\n",
      "\n",
      "\n",
      "Laws--and\n",
      "\n",
      "\n",
      "law--or\n",
      "&\n",
      "&\n",
      "&\n",
      "(&\n",
      "&\n",
      "\n",
      "\n",
      "them--That\n",
      "&\n",
      "expensive--Was\n",
      "&\n",
      "distressing--in\n",
      "&\n",
      "laws--not\n",
      "&\n",
      "meaning--and\n",
      "repeti[ti]on\n",
      "propositions--I\n",
      "was?--telling\n",
      "meeting--which\n",
      "5\n",
      "afternoon--which\n",
      "\n",
      "\n",
      ".\n",
      "&\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_size = 3 # How many adjacent characters in each group considered for the distribution.\n",
    "\n",
    "# Intitalize counts\n",
    "counts = np.zeros((num_glyphs,)*window_size,dtype=np.dtype('u8')) # TODO use scipy sparse array instead\n",
    "\n",
    "# Now go through file and build up distributon\n",
    "with open(filename+\".txt\") as f:\n",
    "    for line in f.readlines():\n",
    "        for word in extract_words(line):\n",
    "            for i in range(len(word)+1):\n",
    "                group = []\n",
    "                for lookback in range(window_size-1,0,-1):\n",
    "                    group.append(glyph_to_int['WORD_START'] if (i-lookback)<0 else glyph_to_int[word[i-lookback]])\n",
    "                group.append(glyph_to_int['WORD_END'] if i>=len(word) else glyph_to_int[word[i]])\n",
    "                counts[tuple(group)] += 1\n",
    "                \n",
    "totals = counts.sum(axis=window_size-1)\n",
    "distribution = counts / (np.vectorize(lambda x : x if x!=0 else 1)(totals.reshape(totals.shape+(1,))))\n",
    "\n",
    "with open(filename+\".pkl\",'wb') as pickle_file:\n",
    "    pickle.dump(distribution,pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word(dist):\n",
    "    previous = [glyph_to_int['WORD_START']]*(window_size-1)\n",
    "    word = []\n",
    "    while previous[-1]!=glyph_to_int['WORD_END']:\n",
    "        if distribution[tuple(previous)].sum()==0:\n",
    "            next_char = np.random.choice(range(num_glyphs))\n",
    "            print(\"Uh oh! This shouldn't happen, right?\")\n",
    "        else:\n",
    "            next_char = np.random.choice(range(num_glyphs),p=distribution[tuple(previous)])\n",
    "        previous = previous[1:]+[next_char]\n",
    "        word.append(next_char)\n",
    "    return ''.join(int_to_glyph[c] for c in word[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dɪskjuzd\n",
      "ɛkʃənfɔɹ\n",
      "ɹɛzən\n",
      "ɪɡnɹ̩z\n",
      "ɪmənsəpɔɹ\n",
      "junəkt\n",
      "kæɹɪkejt\n",
      "ɛvɹ̩mən\n",
      "mæd͡ʒɪst\n",
      "pɹəkənz\n",
      "majtəd\n",
      "fɑlow\n",
      "dɪdɹ̩tiz\n",
      "ækʃənejtə\n",
      "sejtə\n",
      "dutiz\n",
      "pɹups\n",
      "mɑɹmɹ̩ejʃənz\n",
      "hæmpəl\n",
      "sʌt͡ʃ\n",
      "fɔɹdɹ̩səbd͡ʒɛksɛlvz\n",
      "ɑbd͡ʒɛn\n",
      "bɪksɛkspəz\n",
      "ɹɪlətɛvɹ̩i\n",
      "wɛstə\n",
      "fajntɹ̩\n",
      "tɹæŋkwejʃənvɪɹlin\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    word = generate_word(distribution)\n",
    "    if len(word)>4:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next: Look into what words are getting dropped by extract_words, just in case you're still dropping things you shouldn't. Then generate dist (probably with window size of 3 not 4) for english by using wikipedia data and pickle that for later. Have a progress bar for this, Do the same with at least one other language, avoiding the ones that the epitran docs suggest to avoid. Then look into generating phonology and orthography, by using panphon somehow, and try the distribution merging idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1390"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(distribution)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
